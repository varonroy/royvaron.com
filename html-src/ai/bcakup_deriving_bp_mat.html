<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link href="/css-dist/output.css" rel="stylesheet">
	<link href="/css-common/styles.css" rel="stylesheet">
	<title>Roy Varon</title>
</head>

<body class="body-color">

	<div class="article-container">
		{{@ page_navigation(name = 'AI', back = '../pages/ai.html')}}

		<div style="animation-delay: 100ms;" class="fade-in-delay my-10 mx-auto justify-center items-center">
			<p class="article-title-no-link">Deriving Back Propagation in Matrix Form</p>

			<!-- Introduction -->
			<div class="article-paragraph-title">
				Introduction
			</div>

			<div class="article-paragraph">
				Softmax is a very common function in machine learning. It takes a set of real numbers, and converts them
				to a probability distribution.
				\[ \sigma(v)_i = \frac{e^{v_i}}{\sum e^{v_j}} \]
				This article shows how the chain rule can be applied to propagate back derivatives though this function.
			</div>

			<!-- Forward Propagation -->
			<div class="article-paragraph-title">
				Forward Propagation
			</div>

			<div class="article-paragraph">
				Lets consider a portion of a neural network: two layers and the weights between, and an activation
				function {{math('f')}}.Let the first layer have {{math('n')}} neurons and the second one
				{{math('m')}} neurons. Since these two layers are nothing more than arrays of numbers, they can be
				represented using
				vectors:
			</div>

			<div class="article-math-block">
				{{math('first \; layer\: v^1 \in \mathbb{R}^n')}}
			</div>

			<div class="article-math-block">
				{{math('second \; layer\: v^2 \in \mathbb{R}^n')}}
			</div>

			<div class="article-paragraph">
				Let {{math('W_{i,j}')}} be the weight connecting the {{math('i^{th}')}}
				neuron of the second layer and
				the {{math('j^{th}')}}
				neuron of the
				first layer. This way,
				the forward propagation for each of the individual neurons can be written as:
			</div>

			<div class="article-math-block">
				{{math_itex('v^2_i = f\Big(\sum_{k = 1}^{n}W_{i, k} {v^1_k}\Big)')}}
			</div>

			<div class="article-paragraph">
				Or more compactly:
			</div>

			<div class="article-math-block">
				{{math_itex('v^2 = f(W v^1)')}}
			</div>

			<div class="article-paragraph">
				To simplify things, lets split the above step into two:
			</div>

			<div class="article-math-block">
				{{math_itex('s^2 = W v^1')}}
				{{math_itex('v^2 = f(s^2)')}}
			</div>

			<div class="article-paragraph">
				So far we have:
			</div>

			<div class="article-math-block">
				{{math_itex('v^1 \in \mathbb{R}^n')}}
				{{math_itex('s^2 \in \mathbb{R}^m')}}
				{{math_itex('v^2 \in \mathbb{R}^m')}}
				{{math_itex('W \in \mathbb{R}^{m \times n}')}}
				{{math_itex('s^2 = W v^1')}}
				{{math_itex('v^2 = f(s^2)')}}
			</div>

			<!-- Back Propagation -->
			<div class="article-paragraph-title">
				Forward Propagation
			</div>

			<div class="article-paragraph">
				Let the derivative of the second layer with respect to the loss function be given:
			</div>

			<div class="article-math-block">
				{{math_itex('\frac{\partial L}{\partial v^2}')}}
			</div>

			<div class="article-paragraph">
				So the partial derivative of the loss with respect to the {{math('i^{th}')}} neuron in the second layer
				will be
				the {{math('i^{th}')}} element of the above vector:
			</div>

			<div class="article-math-block">
				{{math_itex('\frac{\partial L}{\partial v^2_i} = \Big(\frac{\partial L}{\partial v^2}\Big)_i')}}
			</div>

			<div class="article-paragraph">
				Going a step back, let's write how the {{math('i^{th}')}} element of {{math('s^2')}} is propagation:
			</div>

			<div class="article-math-block">
				{{math_itex('v^2_i = f(s^2_i)')}}
			</div>

			<div class="article-paragraph">
				Then by taking the derivative:
			</div>

			<div class="article-math-block">
				{{math_itex(''\frac{\partial v^2_i}{\partial s^2_i} = f'(s^2_i)'')}}
			</div>

			<div class="article-paragraph">
				We can write the following vector:
			</div>

			<div class="article-math-block">
				{{math_itex(''\frac{\partial v^2}{\partial s^2} = f'(s^2)'')}}
			</div>

			<div class="article-paragraph">
				And apply the chain rule:
			</div>

			<div class="article-math-block">
				{{math_itex(''\frac{\partial L}{\partial s^2_i} = \frac{\partial L}{\partial v^2_i} \frac{\partial
				v^2_i}{\partial s^2_i} = \frac{\partial L}{\partial v^2_i} f'(s^2)'')}}
			</div>

			<div class="article-paragraph">
				And in vector form:
			</div>

			<div class="article-math-block">
				{{math_itex(''\frac{\partial L}{\partial s^2} = \frac{\partial L}{\partial v^2} \circ f'(s^2)'')}}
			</div>

			<div class="article-paragraph">
				(where {{math('\circ')}} represents element wise multiplication). Next, the propagation towards
				{{math('v^1')}} will be
				more complicated,
				because unlike there is no longer a one to one relation between elements in {{math('v^1')}} and
				{{math('s^2')}}. Each element of {{math('v^1')}}
				depends on each element of {{math('s^2')}} in the following way:
			</div>

			<div class="article-math-block">
				{{math_itex(''s^2_i = \sum_{k = 1}^{n} W_{i, k} {v^1_k}'')}}
			</div>

			<div class="article-paragraph">
				So by taking the derivative of the {{math('i^{th}')}} element of {{math('s^2')}} with respect to the
				{{math('j^{th}')}} element of {{math('v^1')}} we get:
			</div>

			<div class="article-math-block">
				{{math_itex('\frac{\partial s^2_i}{\partial v^1_j} = W_{i, j}')}}
			</div>

			<div class="article-paragraph">
				This means that when we apply the chain rule to get {{math('\partial L / \partial v^1_i')}}, we need to
				consider how every element
				of {{math('s_2')}} will impact the total derivative:
			</div>

			<div class="article-math-block">
				{{math_itex(''\frac{\partial L}{\partial v^1_i} = \sum_{j = 0}^{m}\frac{\partial L}{\partial s^2_j}
				\frac{\partial s^2_j}{\partial v^1_i} = \sum_{j = 0}^{m}\frac{\partial L}{\partial s^2_j} W_{j, i}'')}}
			</div>

			<div class="article-paragraph">
				Notice how the indices in {{math('W')}} are flipped. We can flip the indices back by transposing the
				matrix:
			</div>

			<div class="article-math-block">
				{{math_itex(''\frac{\partial L}{\partial v^1_i} = \sum_{j = 0}^{m}\frac{\partial L}{\partial s^2_j}
				(W^T)_{i, j} =
				\sum_{j = 0}^{m} (W^T)_{i, j} \frac{\partial L}{\partial s^2_j}'')}}
			</div>

			<div class="article-paragraph">
				So the {{math('i^{th}')}} element of {{math('\partial L / \partial v^1_i')}} equals to the dot product
				of the {{math('i^{th}')}} row of
				matrix {{math('W^T')}} with the vector {{math('\partial L / \partial s^2_j')}}. This is exactly what
				matrix
				multiplication does, so we can write:
			</div>

			<div class="article-math-block">
				{{math_itex(''\frac{\partial L}{\partial v^1} = W^T \frac{\partial L}{\partial s^2}'')}}
			</div>

			<div class="article-paragraph">
				Finally, we need to calculate the derivatives with respect to each of the weights in the matrix
				{{math('W')}}.
				Using the above forward propagation formula:
			</div>

			<div class="article-math-block">
				{{math_itex(''s^2_i = \sum_{k = 1}^{n}W_{i, k} {v^1_k} '')}}
			</div>

			<div class="article-paragraph">
				We can find the partial derivative:
			</div>

			<div class="article-math-block">
				{{math_itex('' \frac{\partial s^2_i}{\partial W_{i, j}} = v^1_j'')}}
			</div>

			<div class="article-paragraph">
				Notice that the index of the element of {{math('s^2')}} has to match the index of the row of
				{{math('W')}}.
				Since only the
				{{math('i^{th}')}} row has an effect on that element.
			</div>

			<div class="article-paragraph">
				And now by applying the chain rule:
			</div>

			<div class="article-math-block">
				{{math_itex('' \frac{\partial L}{\partial W_{i, j}} = \frac{\partial L}{\partial s^2_i} \frac{\partial
				s^2_i}{\partial
				W_{i, j}} = \frac{\partial L}{\partial s^2_i} v^1_j '')}} </div>

			<div class="article-paragraph">
				It is a bit hard to see at first, but we can rewrite this expression as a multiplication of a column
				vector
				by a row vector:
			</div>

			<div class="article-math-block">
				{{math_itex('' \frac{\partial L}{\partial W} = \frac{\partial L}{\partial s^2} (v^1)^T '')}}
			</div>

			<div class="article-paragraph">
				Here is a small example of how that multiplication would work:
			</div>

			<div class="article-math-block">
				{{math_itex(''
				\begin{bmatrix}
				a_1\\
				a_2
				\end{bmatrix}
				\begin{bmatrix}
				b_1 & b_2 & b_3
				\end{bmatrix}
				=
				\begin{bmatrix}
				a_1b_1 & a_1b_2 & a_1b_3\\
				a_2b_1 & a_2b_2 & a_2b_3\\
				\end{bmatrix}
				'')}}
			</div>

			<!-- Summary -->
			<div class="article-paragraph-title">
				Summary
			</div>

			<div class="article-paragraph">
				To Summarize, given the following structure of a neural network:
			</div>

			<div class="article-math-block">
				{{math_itex(''v^1 \in \mathbb{R}^n '')}}
				{{math_itex(''s^2 \in \mathbb{R}^m '')}}
				{{math_itex(''v^2 \in \mathbb{R}^m '')}}
				{{math_itex(''W \in \mathbb{R}^{m \times n} '')}}
				<br>
				{{math_itex(''s^2 = W v^1 '')}}
				{{math_itex(''v^2 = f(s^2) '')}}
			</div>

			<div class="article-paragraph">
				And the derivative of the loss function with respect to the last layer:
			</div>

			<div class="article-math-block">
				{{math_itex(''\frac{\partial L}{\partial v^2}'')}}
			</div>

			<div class="article-paragraph">
				We can back propagate the derivative with the following formulas:
			</div>

			<div class="article-math-block">
				{{math_itex(''\frac{\partial L}{\partial s^2} = \frac{\partial L}{\partial v^2} \circ f'(s^2) '')}}
				{{math_itex(''\frac{\partial L}{\partial v^1} = W^T \frac{\partial L}{\partial s^2}'')}}
				{{math_itex(''\frac{\partial L}{\partial W} = \frac{\partial L}{\partial s^2} (v^1)^T '')}}
			</div>
		</div>
	</div>

	<!-- mathjax -->
	<!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> -->
</body>

</html>