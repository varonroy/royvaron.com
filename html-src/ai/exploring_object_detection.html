<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link href="/css-dist/output.css" rel="stylesheet">
	<link href="/css-common/styles.css" rel="stylesheet">
	<title>Roy Varon</title>
</head>

<body class="body-color">

	<div class="article-container">
		{{@ page_navigation(name = 'AI', back = '../pages/ai.html')}}

		<div style="animation-delay: 100ms;" class="fade-in-delay article-sub-container">
			{{@ article_title(title = 'Exploring Object Detection')}}

			<!-- Introduction -->
			<div class="article-paragraph-title">
				Introduction
			</div>

			<div class="article-paragraph">
				This article will explore several object detection neural networks,
				datasets, training techniques and evaluation measures.
			</div>

			<!-- Mnist OD Dataset -->
			<div class="article-paragraph-title">
				Mnist OD Dataset
			</div>

			<div class="article-paragraph">
				<a class="text-link"
					href="https://github.com/Cringere/exploring_object_detection/blob/master/custom_datasets/mnist_od_dataset.py">code</a>
			</div>

			<div class="article-paragraph">
				Mnist is a famous dataset for object classification. It contains
				many hand-drawn digits all in 28x28 images. In object classification,
				this dataset is considered by many the easiest one to classify -
				if a network can't classify Mnist, there is no way it could classify
				a more complex dataset such as ImageNet.
			</div>

			<div class="article-paragraph">
				We can create an analogous "easiest" object detection dataset by
				preparing a relatively large canvas, say 128x128 and scatter on
				it digits from mnist. The same argument could then be made - if
				an object detection network can't detect these digits - there is
				no way it could detect more complex objet on more complex datasets,
				like Pascal.
			</div>

			<div class="article-paragraph">
				Here is an example of how an item from that dataset would look like
				(image and label):
			</div>

			<img class="glsl-grids-img" src="/res/exploring_object_detection/mnist_od_data_item.png"
				alt="mnist OD dataset item">

			<div class="output-block">
				{{@ ai/exp_od/img_1_labels_1}}
			</div>

			<!-- Network IO -->
			<div class="article-paragraph-title">
				Network IO
			</div>

			<div class="article-paragraph">
				This article follows YOLO's style of network architectures and
				loss design. Each input image is segmented into a grid of cells.
				For every cell, the network predicts a specific amount of bounding
				boxes. The amount of bounding boxes and the number of cells are
				hyper parameters. Each box contains the center \((x, y)\) and size
				\((w, h)\) relative to the cell of where it thinks an object exists
				as well as a probability of there being an object there at all.
				The method of predicting the class of the object varies from model
				to model.
			</div>

			<div class="article-paragraph">
				for example the output shape with \(c_a \times c_b\) cells, \(c\)
				classes, and \(b\) boxes per cell would be \((c_a, c_b, b, c + 5)\).
				The \(+5\) is for the coordinates, size, and probability.
				Alternately, a different model could output a single class prediction
				per cell, in contrast to each class prediction per box. Then the
				output shape would be \((c_a, c_b, c + 5b)\).
			</div>

			<div class="article-paragraph">
				Similarly, if the model expects only zero or one objects per cell,
				then the target shape would be \((c_a, c_b, c + 5)\). If the model
				expects more than one object per cell, then the label would have
				to be repeated for the maximum amount of allowed objects per cell:
				\((c_a, c_b, (c + 5) * b)\).
			</div>


			<div class="article-paragraph">
				The number of cells is a hyper parameter of the model, not the dataset,
				so object detection datasets contain their absolute coordinates
				and sizes in pixels. Additionally, these datasets would store their
				bounding boxes as lists, not tensors. For these reasons, before
				training, the raw dataset's labels need to be converted to relative
				cell sizes, and then to tensors. For example (canvas size is
				\(256 \times 256 \)):
			</div>

			<img class="glsl-grids-img" src="/res/exploring_object_detection/ex_item.png" alt="mnist OD dataset item">

			<div class="output-block">
				{{@ ai/exp_od/img_2_labels_1}}
			</div>

			<div class="output-block">
				{{@ ai/exp_od/img_2_labels_2}}
			</div>

			<!-- Yolo-V1 Loss -->
			<div class="article-paragraph-title">
				Yolo-V1 Loss
			</div>

			<div class="article-paragraph">
				\[
				\begin{align}
				\text{loss} =
				& {{@ lambda(sub_text = 'coord')}}
				{{@ ai/exp_od/yolo_loss_pos}} \\
				& +
				{{@ lambda(sub_text = 'coord')}}
				{{@ ai/exp_od/yolo_loss_size}} \\
				& +
				{{@ ai/exp_od/yolo_loss_obj_prob}} \\
				& +
				{{@ lambda(sub_text = 'noobj')}}
				{{@ ai/exp_od/yolo_loss_noobj_prob}} \\
				& +
				{{@ ai/exp_od/yolo_loss_classes}}
				\end{align}
				\]
			</div>

			<div class="article-paragraph">
				Where \(x_i, y_i, w_i, h_i\) are the coordinates and sizes of the
				predicted boxes. \(\hat{x_i}, \hat{y_i}, \hat{w_i}, \hat{h_i}\)
				are the target boxes.
			</div>

			<div class="article-paragraph">
				\(C_i\) is the predicted probability of an object being in the bounding
				box of cell \(i\). \(\hat{C_i}\) is the target probability. When
				there is an object, \(\hat{C_i}\), and when there isn't \(\hat{C_i}=0\).
			</div>

			<div class="article-paragraph">
				\(\mathbb{1}_{ij}^\text{obj}\) is an identity function: \(1\) when cell \(i\)
				has a target box, and bounding box \(j\) is responsible for predicting
				it, \(0\) otherwise. \(\mathbb{1}_{ij}^\text{noobj}\) is the opposite: \(1 -
				\mathbb{1}_{ij}^\text{obj}\).
			</div>

			<div class="article-paragraph">
				\(\mathbb{1}_i^\text{obj}\) is also an identity function: \(1\) when cell \(i\)
				has a target box, \(0\) otherwise.
			</div>

			<div class="article-paragraph">
				\(p_i(c)\) is the predicted probability of class \(c\).
				\(\hat{p_i(c)}\) is the target probability of class \(c\).
				\(\hat{p_i(c)}\) is either \(0\) or \(1\).
			</div>


			<div class="article-paragraph">
				Notice the difference between \(C_i\) and \(p_i(c)\). First, \(C_i\)
				is used to determine if there is an objet at all or not, and then
				\(p_i(c)\) is used to determine what class the object belongs to.
			</div>

			<div class="article-paragraph">
				In cells that contain objects, the predicted box that counts as
				being the responsible one for the prediction, is the one that has
				the highest IOU (intersection over union) with the target box.
				Unlike inference, the selected predicted box for each cell is a
				function of both the network's output and the target label.
			</div>

			<img class="glsl-grids-img" src="/res/exploring_object_detection/iou.png" alt="iou">

			<!-- Yolo Loss Implementation -->
			<div class="article-paragraph-title">
				Yolo Loss Implementation
			</div>

			<div class="article-paragraph">
				{{@ links/yolo_git}}
			</div>

			<div class="article-paragraph">
				The biggest challenge in implementing the Yolo loss is writing it
				with as many vectorized operations, avoiding explicit control flow.
				There are many ways of doing so, and the method described in this
				section is just one of them.
			</div>

			<div class="article-paragraph-note">
				Note:
				<br>
				In the Yolo loss the cells are flattened and indexed from
				\(0\) to \(S^2\). In this implementation the cells are left in
				their grid form and indexed from \((0, 0)\) to \((S, S)\).
			</div>

			<div class="article-paragraph">
				The predicted boxes tensor has the shape:
				\[
				\text{pred}: (\text{batch}, \text{cells_a}, \text{cells_b}, \text{classes} + 5b)
				\]
				The label boxes tensor has the shape:
				\[
				\text{label}: (\text{batch}, \text{cells_a}, \text{cells_b}, \text{classes})
				\]
			</div>

			<div class="article-paragraph">
				First, split all the cells into ones that have labeled objects and
				ones that don't. This is similar to the \(\mathbb{1}_i^\text{obj}\).
				{{@ ai/exp_od/code_01}}
			</div>

			<div class="article-paragraph-note">
				Note:
				<br>
				since the classes's indices range from \(0\) to \(\text{classes} - 1\),
				the probability's index is \(\text{classes}\). The number \(0.5\)
				is arbitrary here, since the label's probabilities are always \(0\)
				or \(1\).
			</div>

			<div class="article-paragraph">
				Then the cells are extracted:
				{{@ ai/exp_od/code_02}}
				This operation will flatten the tensors such that the new shapes
				will be:
				\[
				\begin{align}
				\text{pred_obj}: (\text{x}, \text{classes} + 5b) \\
				\text{label_obj}: (\text{x}, \text{classes} + 5)
				\end{align}
				\]
			</div>

			<div class="article-paragraph">
				The same can be done for cells that don't have cells. This will
				be similar to the \(\mathbb{1}_i^\text{noobj}\) function:
				{{@ ai/exp_od/code_03}}
				And the shapes:
				\[
				\begin{align}
				\text{pred_noobj}: (\text{y}, \text{classes} + 5b)\\
				\text{label_noobj}: (\text{y}, \text{classes} + 5)
				\end{align}
				\]
				Such that \(\text{x} + \text{y} = \text{batch} \cdot \text{cells_a} \cdot \text{cells_b} \)
				since every cell either has an object or doesn't.
			</div>

			<div class="article-paragraph">
				Since all the predicted boxes for each cell share the same class
				probabilities it can be helpful to separate them:
				{{@ ai/exp_od/code_04}}
				And the resulting shapes:
				\[
				\text{pred_obj_classes}: (\text{x}, \text{classes})
				\]
				\[
				\text{pred_obj_classes}: (\text{x}, 5b)
				\]
				\[
				\text{label_obj_classes}: (\text{x}, \text{classes})
				\]
				\[
				\text{label_obj_boxes}: (\text{x}, 5b)
				\]
			</div>

			<div class="article-paragraph">
				The next step is to reduce\((\text{x}, 5b)\) into \((\text{x}, 5)\)
				by choosing the responsible box.
				As stated in the previous section, the responsible box from each of
				the \(b\) predicted ones is the one with the highest IOU with respect.
				to the label box. The IOU calculation consists out of a few simple
				arithmetic operations which means its input tensors must have the
				same shapes.
			</div>

			<div class="article-paragraph">
				{{@ links/iou_git}}
			</div>

			<div class="article-paragraph">
				One way matching between the predictions and the labels is to first,
				reshape the predicted boxes such that each box is on its own vector,
				and repeat the label boxes such that each predicted box will have
				its corresponding duplicated label box.
				\[
				\text{pred_obj_boxes}: (\text{x}, 5b) \rightarrow (\text{x}, b, 5)
				\]
				\[
				\text{label_obj_boxes}: (\text{x}, 5) \rightarrow (\text{x}, 1, 5) \rightarrow (\text{x}, b, 5)
				\]
			</div>

			<div class="article-paragraph">
				then, the indices of the boxes with highest iou are calculated and
				stored in a vector using \(\arg\max\):
				{{@ ai/exp_od/code_05}}
				\[
				\text{indices}: (\text{x}, )
				\]
			</div>

			<div class="article-paragraph">
				Getting the actual boxes from the indices is a bit more complicated.
				The goal is to create a tensor such that:
				{{@ ai/exp_od/code_06}}
				The accompanying code to this article is written in a {{@ links/pytorch}} which
				has a similar function: {{@ links/pytorch_gather}}. {{@ links/pytorch_gather}} allows to perform
				the following action:
				{{@ ai/exp_od/code_07}}
				Which is quite similar to what is actually needed. Specifically,
				if the indices are reshaped to add another dimension and then repeated
				on their last dimension:
				\[
				\text{indices}: (\text{x}, ) \rightarrow (\text{x}, 1, 1) \rightarrow (\text{x}, 1, 5)
				\]
				they match exactly the format required by {{@ links/pytorch_gather}}.
				The result would a tensor containing the boxes with the maximum ious,
				which can be easily reshaped to remove the unnecessary dimension.
				\[
				\text{pred_obj_boxes_max}: (\text{x}, 1, 5) \rightarrow (\text{x}, 5)
				\]
			</div>

			<div class="article-paragraph">
				Finally the losses can be calculated (\(\text{mse}\) stands for mean squared
				loss with a summation reduction):
				\[
				{{@ ai/exp_od/yolo_loss_pos}}
				\]
				{{@ ai/exp_od/code_08}}
			</div>

			<div class="article-paragraph">
				The sizes loss is slightly more complex because it has a squared
				root term. If nothing stops the network from outputting negative
				values, a division error will occur. In the yolo paper, the square
				root is as an absolute scale factor so it doesn't matter if the number
				is negative or not - its absolute value needs to be scaled.
				\[
				\sum_{i=0}^{S^2}
				\sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} \Big[
				\big(\sqrt{w_i} - \sqrt{\hat{w_i}} \big) ^ 2 +
				\big(\sqrt{h_i} - \sqrt{\hat{h_i}} \big) ^ 2
				\Big]
				\]
				{{@ ai/exp_od/code_09}}
			</div>

			<div class="article-paragraph">
				The probability is just like the location loss:
				\[
				\sum_{i=0}^{S^2}
				\sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} \Big(
				C_i - \hat{C_i}
				\Big) ^ 2
				\]
				{{@ ai/exp_od/code_10}}
			</div>

			<div class="article-paragraph">
				The probability for the cells without objects is different since
				all the boxes they contain need to be added to the loss. The probabilities
				indices of these boxes are \(\text{classes}, \text{classes} + 5, ..., \text{classes} + 5b\)
				In {{@ links/pytorch}}, specific indices can be extracted using the \(\text{index_select}\) function:
				\[
				\sum_{i=0}^{S^2}
				\sum_{j=0}^{B} \mathbb{1}_{ij}^\text{noobj} \Big(
				C_i - \hat{C_i}
				\Big) ^ 2
				\]
				{{@ ai/exp_od/code_11}}
			</div>

			<div class="article-paragraph">
				The final item is very similar to the location loss, but instead
				of having the identity function over cells and boxes, the identity
				function is over cells only:
				\[
				\sum_{i=0}^{S^2} \mathbb{1}^\text{obj}
				\sum_{c\in \text{classes}} \Big(
				p_i(c) - \hat{p}_i(c)
				\Big) ^ 2 =
				\]
				{{@ ai/exp_od/code_12}}
			</div>

			<!-- First Model -->
			<div class="article-paragraph-title">
				First Model
			</div>

			<div class="article-paragraph">
				<a class="text-link"
					href="https://github.com/Cringere/exploring_object_detection/blob/master/mnist_od/net.py">architecture</a>
				<br>
				<a class="text-link"
					href="https://github.com/Cringere/exploring_object_detection/blob/master/mnist_od_train.py">training</a>
				<br>
				<a class="text-link"
					href="https://github.com/Cringere/exploring_object_detection/blob/master/mnist_od_test.py">testing</a>
			</div>

			<div class="article-paragraph">
				This model is built of a series of same-padding convolutions of
				various sizes, 1x1 convolutions and max pool layers. Its inputs
				are batches of images with shape \((1, 128, 128)\) and its output
				is a batch of predicted bounding boxes \((4, 4, 20)\), where \(4\)
				is the number of cells per row and columns, there are \(10\) classes,
				and \(2\) predicted boxes per cell. The output values of the network
				that were responsible for probabilities - box confidence and class
				probabilities were also ran through a sigmoid.
			</div>

			<div class="article-paragraph">
				The network was trained on the mnist-od dataset (presented above)
				with \(16392\) items, batch size of \(64\) and \(10\) epochs. Here
				are the losses (excluding the first few hundred iterations):
			</div>

			<img class="mx-auto" style="width: 32rem;" src="/res/exploring_object_detection/mnist_od_losses_offset.png"
				alt="mnist OD first network training">

			<div class="article-paragraph">
				Testing the dataset (red are target boxes, green are predicted
				boxes with \(>70\%\) confidence):
			</div>

			<img class="w-96 mx-auto m-3" src="/res/exploring_object_detection/mnist_od_sample.png"
				alt="mnist OD first network training">

			<!-- Model evaluation - mAP -->
			<div class="article-paragraph-title">
				Model evaluation - mAP
			</div>

			<div class="article-paragraph">
				<a class="text-link"
					href="https://github.com/Cringere/exploring_object_detection/blob/master/mnist_od_stats.py">code</a>
				<br>
				<a class="text-link"
					href="https://github.com/Cringere/exploring_object_detection/blob/master/utils/__init__.py">mAP</a>
			</div>

			<div class="article-paragraph">
				Sampling a few images from the dataset and visually drawing their
				bounding boxes is a great tool for visualization, but it doesn't
				provide a concrete metric for how well the model is doing. For that,
				there exists the mean average precision (mAP) metric.
			</div>

			<div class="article-paragraph">
				mAP relies on two key concepts - precision and recall. Precision
				is the number of true positive predictions over the sum of both
				the true positives and the false positives. Basically, its the
				number of correctly predicted boxes out of all the predictions.
				<br>
				Recall is the number of true positive predictions over the sum of
				both the true positive and the false negative predictions - which
				are basically the number of target boxes.
			</div>

			<div class="article-paragraph">
				mAP is calculated for a specific IOU threshold. When a predicted box
				and a target box have an iou thats about the threshold, the target
				box counts as a true positive, otherwise it is counted as a false
				positive.
			</div>

			<img class="w-96 mx-auto m-3" src="/res/exploring_object_detection/tp_fp.png"
				alt="mnist OD first network training">

			<div class="article-paragraph">
				in mAP there exists another threshold - the confidence threshold.
				This threshold controls which boxes count as predictions and which
				are ignored.
			</div>

			<div class="article-paragraph">
				The fist step in calculating the mAP is plotting a precision vs
				recall graph. Each point on that graph, is calculated using a
				specific confidence threshold. The idea is that starting from
				\(100%\) the confidence threshold is gradually lowered to \(0%\)
				while the precision and recall calculated.
			</div>

			<div class="article-paragraph">
				As long as as the range \([0\%, 100\%]\) is segmented into enough
				points, it doesn't really matter how it is segmented. However, one
				efficient way of choosing these segments is to sort all the predicted
				boxes from highest confidence to lowest and using their confidences
				as the thresholds. This way, each time the threshold is lowered, only
				one additional box changes its status from being an ignored box
				to a predicted box.
			</div>

			<div class="article-paragraph">
				Here is are the plots for precision and confidence threshold vs
				recall for each of the classes and each several ious. The additional
				line of the thresholds is not necessary but visually useful, it
				allows you to see what the precision and recall were at each specific
				threshold.
			</div>

			<img class="mx-auto m-3" style="width: 32rem;" src="/res/exploring_object_detection/mnist_od_stats.png"
				alt="mnist OD stats">

			<div class="article-paragraph">
				These graphs are calculated for a specific IOU and class. Average
				precision is the area under these graphs. The mean average precision
				is the is calculated by averaging those areas with respect to the
				classes. Plotting the mAP vs IOU produces the following graph:
			</div>

			<img class="mx-auto w-96" style="width: 32rem;"
				src="/res/exploring_object_detection/mnist_od_mean_average_precision.png" alt="mnist OD mAP">

			<!-- Second Model -->
			<div class="article-paragraph-title">
				Second Model
			</div>

			<div class="article-paragraph">
				<a class="text-link"
					href="https://github.com/Cringere/exploring_object_detection/blob/master/voc_od/nets.py">architecture</a>
				<br>
				<a class="text-link"
					href="https://github.com/Cringere/exploring_object_detection/blob/master/voc_od_train.py">training</a>
			</div>

			<div class="article-paragraph">
				The goal of the second model is to work detect objects in the {{@ links/pascal_voc}}
				dataset. Compared to the mnist od dataset, this is dataset contains
				real life images with real life objects to detect. Obviously this
				task is much more difficult than the previous one.
			</div>

			<div class="article-paragraph">
				When dealing with extremely difficult tasks, it is sometimes easier
				to first train a network on an easier one, and then transfer it
				(with some differences) to the new task. This network has three
				parts - a base, a classification head, and a detection head. when
				the base is paired with the classification head, the networks output,
				is a one-hot vector - a common classification encoding. When the
				base is paired with the detection head, the networks output is a
				a list of bounding boxes, just as in the previous network.
			</div>

			<div class="article-paragraph">
				During the training process, a base and a classification head are
				both trained on {{@ links/image_net}}. Then, the classification head
				is thrown away, and the base is paired with a detection head. This
				is the final architecture of the network is it is trained until
				a sufficiently small loss.
			</div>

			<!-- Second Model evaluation - mAP -->
			<div class="article-paragraph-title">
				Second Model evaluation - mAP
			</div>

			<div class="article-paragraph">
				<a class="text-link"
					href="https://github.com/Cringere/exploring_object_detection/blob/master/voc_od_map_derivation.ipynb">derivation</a>
				<br>
				<a class="text-link"
					href="https://github.com/Cringere/exploring_object_detection/blob/master/voc_od_stats.py">statistics</a>
			</div>

			<div class="article-paragraph">
				Calculating the mAP for this model is very similar to the previous.
				In the previous model, in order for a predicted box count as a true
				positive its center needed to be in the same cell as its target.
				This is a simplification that worked in the previous model because
				all objects were roughly the same size and aspect ratio. In this
				model, the bounding boxes need to be converted from their relative
				coordinates systems (the cell's coordinate system) to absolute system
				(absolute in terms of the whole image).
			</div>

			<div class="article-paragraph">
				This difference creates an extra complication. Now, each image in the
				batch has a different amount of predicted boxes and labels, and
				the iou of each pair of prediction and target needs to be calculated.
				For a detailed walk through the algorithm consult the "derivation"
				link above.
			</div>

			<div class="article-paragraph">
				Here are some result after training the network (not to completion):
			</div>

			<img class="w-96 mx-auto m-3" src="/res/exploring_object_detection/voc_od_test_sample.png"
				alt="voc OD first network training">

			<br>

			<img class="w-96 mx-auto m-3" style="width: 32rem;" src="/res/exploring_object_detection/voc_od_stats.png"
				alt="voc OD statistics">

			<br>

			<img class="w-96 mx-auto m-3" style="width: 32rem;"
				src="/res/exploring_object_detection/voc_od_mean_average_precision.png"
				alt="voc OD mean average precision">

		</div>
	</div>


	<!-- mathjax -->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<!-- highlight.js -->
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
	<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
	<script>hljs.highlightAll();</script>

	<!-- <pre><code class="language-plaintext">...</code></pre> -->
	<!-- <pre><code class="nohighlight">...</code></pre> -->
</body>

</html>